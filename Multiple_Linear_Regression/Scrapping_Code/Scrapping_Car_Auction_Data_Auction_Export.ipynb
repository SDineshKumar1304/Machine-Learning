{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Scrapping Car Auction Data - Auction Export",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "***Problem Statement from Kaggle***\n",
        "\n",
        "https://www.kaggle.com/code/aneesds/scrapping-car-auction-data-auction-export\n",
        "\n",
        "\n",
        "This notebook is a preparation step to prepare and scrap data for our Car Auction Price prediction project. In this notebook we are scraping data from https://www.theaa.com.\n"
      ],
      "metadata": {
        "id": "qoB6MNWFSAHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import pandas as pd\n",
        "import requests as rq\n",
        "from bs4 import BeautifulSoup as bs"
      ],
      "metadata": {
        "id": "e-iE8MVBSAHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create two empty lists to take the different features of the car.\n",
        "# The first will take the name and the price, the second will take the other features.\n",
        "\n",
        "first_list = []\n",
        "second_list = []\n",
        "\n",
        "# create a loop to get the car deatils from multiple pages - pages 1 to 150\n",
        "\n",
        "for i in range(1, 150):\n",
        "    website = \"https://www.theaa.com/used-cars/displaycars?sortby=datedesc&page=\" + str(i) + \"&pricefrom=3000&priceto=30000\"\n",
        "    page = rq.get(website)\n",
        "    soup = bs(page.content, 'html.parser')\n",
        "\n",
        "    # get the first features of the cars\n",
        "    name = soup.select('.make-model-text')\n",
        "    price = soup.find_all(\"strong\",{\"class\": \"total-price\"})\n",
        "\n",
        "    #store the first features in a list and then create a dataframe\n",
        "    for name, price in zip(name, price):\n",
        "        first_list.append([name.text, price.text])\n",
        "        first_features = pd.DataFrame(first_list, columns = ['name', 'price'])\n",
        "\n",
        "    # get the other feaures of the cars\n",
        "    other_features = soup.find_all('ul', {'class':\"vl-specs\"})\n",
        "\n",
        "    # loop through the html element to scrape the required features,append to the second list and\n",
        "    # store in a dataframe\n",
        "    for feature in other_features:\n",
        "        my_dict = {}\n",
        "        feature = feature.find_all('li')\n",
        "        my_dict['year'] = feature[0].text\n",
        "        my_dict['mileage'] = feature[2].text\n",
        "        my_dict['engine'] = feature[4].text\n",
        "\n",
        "        try:\n",
        "            my_dict['transmission'] = feature[6].text\n",
        "        except:\n",
        "            my_dict['transmission'] = 'n/a'\n",
        "\n",
        "        second_list.append(my_dict)\n",
        "\n",
        "\n",
        "    second_features = pd.DataFrame(second_list, columns = ['year', 'mileage', 'engine', 'transmission'])\n",
        "\n",
        "# concatenate the two dataframes and rename the columns\n",
        "cars = pd.concat([first_features, second_features], axis=1,ignore_index=True)\n",
        "cars.columns = ['name', 'price', 'year', 'mileage', 'engine', 'transmission']\n",
        "\n",
        "# save new dataframe in a csv file\n",
        "cars.to_csv('car_data.csv', index=False)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-07T10:24:26.780266Z",
          "iopub.execute_input": "2021-12-07T10:24:26.780545Z",
          "iopub.status.idle": "2021-12-07T10:27:20.409503Z",
          "shell.execute_reply.started": "2021-12-07T10:24:26.780517Z",
          "shell.execute_reply": "2021-12-07T10:27:20.408548Z"
        },
        "trusted": true,
        "id": "_bxtPDN9SAHh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}